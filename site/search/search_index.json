{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Background","text":"<p>Large language models offer state-of-the-art performance across many NLP tasks, but their massive size poses significant challenges for efficient deployment. Quantization techniques help reduce memory and computation cost by compressing model weights into low-bit representations. Among these, QTIP (Quantization with Trellises and Incoherence Processing) stands out as a recent method that combines Gaussianization and trellis-based encoding to achieve high-quality 2\u20134 bit quantization. For more details, see the QTIP paper: QTIP: Quantization with Trellises and Incoherence Processing.</p> <p>At the same time, vLLM is an open-source, high-throughput inference engine designed to serve LLMs efficiently using features like PagedAttention and kernel fusion. However, it does not natively support advanced quantization methods like QTIP.</p> <p>This project focuses on integrating QTIP into vLLM\u2019s inference pipeline. The work includes decoding the QTIP compression logic, writing a full Python implementation, modifying vLLM\u2019s linear layer to support QTIP-based quantized weights and testing. The result is a hybrid system that combines algorithmic innovation with systems-level engineering, enabling lower latency inference while maintaining model accuracy.</p>"},{"location":"1_decoding_methods/","title":"QTIP Decoding Methods","text":"<p>After quantization using the bitshift trellis, QTIP decodes compressed representations into approximate floating-point weights at inference time. Unlike traditional decoding methods which rely on explicit codebooks or tables, QTIP supports multiple decoding strategies, including lookup-free, hardware-efficient, and hybrid techniques.</p> <p>This section explains the core logic of each decoding method implemented in QTIP.</p>"},{"location":"1_decoding_methods/#1-lookup-table-lut","title":"1. Lookup Table (LUT)","text":"<p>Method: Store a full codebook of all possible output values, and use the quantized code (e.g., 4-bit index) to retrieve the value.</p> <p>Example: For a 4-bit weight encoding:</p> <pre><code>lut = {\n    0b0000: -2.0,\n    0b0001: -1.5,\n    ...\n    0b1111: 2.0\n}\ndecoded = lut[code]\n</code></pre> <p>Characteristics: - Fastest decoding - High memory usage (1 table per block/channel) - Precomputed, no computation required at inference</p> <p>LUT-based decoding is conceptually straightforward, but expensive when the codebook size is large or memory is constrained.</p>"},{"location":"1_decoding_methods/#2-1mad-one-multiply-add-algorithm-1","title":"2. 1MAD: One Multiply-Add (Algorithm 1)","text":"<p>Goal: Approximate a Gaussian-distributed weight using only simple arithmetic operations \u2014 no table required.</p>"},{"location":"1_decoding_methods/#steps","title":"Steps:","text":"<ol> <li>Use a linear congruential generator (LCG) to pseudorandomize the compressed index.</li> <li>Interpret the 32-bit LCG result as four 8-bit integers.</li> <li>Sum the 4 components \u2014 the result resembles a Gaussian (by central limit theorem).</li> <li>Apply a scale and shift to normalize.</li> </ol> <p>Pseudocode:</p> <pre><code>x = (a * index + b) % 2^32\nx = (x &amp; 255) + ((x &gt;&gt; 8) &amp; 255) + ((x &gt;&gt; 16) &amp; 255) + ((x &gt;&gt; 24) &amp; 255)\nx = (x - 510) / 147.8  # Normalize to N(0,1)\n</code></pre> <p>Parameters: - a = 34038481 - b = 76625530</p> <p>Features: - Only 2\u20133 instructions per weight - Fully LUT-free - Produces approximately Gaussian weights</p>"},{"location":"1_decoding_methods/#3-3inst-three-instruction-decode-algorithm-2","title":"3. 3INST: Three-Instruction Decode (Algorithm 2)","text":"<p>Goal: Improve decoding resolution and Gaussianity using a small number of hardware-friendly operations.</p>"},{"location":"1_decoding_methods/#steps_1","title":"Steps:","text":"<ol> <li>Run an LCG to generate a 32-bit pseudorandom number X.</li> <li>XOR bits of X with a carefully constructed float16 \"template\" (m) to inject entropy into mantissa, exponent, and sign.</li> <li>Reinterpret the result as two float16s, sum them.</li> </ol> <p>Pseudocode (simplified):</p> <pre><code>X = (a * index + b) % 2^32\nm = 0.922 (in float16)\n# Pack and XOR with both halves of X\nx1 = reinterpret((X &amp; 0xFFFF) ^ mask(m))\nx2 = reinterpret((X &gt;&gt; 16) ^ mask(m))\ndecoded = x1 + x2\n</code></pre> <p>Parameters: - a = 89226354 - b = 64248484 - m = 0.922 (float16)</p> <p>Notes: - Results approximate the sum of two mirrored exponential variables \u2192 close to Gaussian - Achieves higher entropy than 1MAD - Can be implemented in 3 ALU instructions on modern GPUs</p>"},{"location":"1_decoding_methods/#4-hyb-hybrid-decode-algorithm-3","title":"4. HYB: Hybrid Decode (Algorithm 3)","text":"<p>Goal: Combine the flexibility of a small LUT with the randomness of a hash function for high-quality decoding.</p>"},{"location":"1_decoding_methods/#core-idea","title":"Core Idea:","text":"<ul> <li>Use a hashed version of the index to look up a 2D vector from a small shared LUT.</li> <li>Then apply a sign flip to increase representational range.</li> </ul>"},{"location":"1_decoding_methods/#steps_2","title":"Steps:","text":"<ol> <li>Compute a fast hash:    <code>python    x = x * x + x</code></li> <li>Use bits 14 - Q + 1 to 14 as index into a 2D LUT of size 2^Q \u00d7 2.</li> <li>Flip the sign of the second float in the 2D vector if bit 15 is 1.</li> </ol> <p>Pseudocode:</p> <pre><code>x = (index * index + index) % 2^32\ni = (x &gt;&gt; (15 - Q)) &amp; (2^Q - 1)\nv = LUT[i]  # returns [v0, v1]\nif x &amp; (1 &lt;&lt; 15):\n    v[1] = -v[1]\ndecoded = v\n</code></pre> <p>Notes: - Amortized ~2 instructions per weight - LUT can be fine-tuned offline via K-means - Fits in L1/shared memory on GPUs - Supports quantizing V=2 weight groups jointly</p>"},{"location":"1_decoding_methods/#5-configuration-in-qtip","title":"5. Configuration in QTIP","text":"<p>Each decoding method can be selected with a simple config field in <code>quantize_config.json</code>:</p> <pre><code>{\n  \"decode_mode\": \"lut\"  // or \"1mad\", \"3inst\", \"hyb\"\n}\n</code></pre> <p>Changing this mode automatically triggers the corresponding decoding logic during model inference.</p>"},{"location":"1_decoding_methods/#comparison-of-decoding-methods","title":"Comparison of Decoding Methods","text":"Method Memory Usage Computation Quality Best For LUT High Minimal Exact Small models, memory-rich environments 1MAD None Low (2-3 ops) Good Memory-constrained, compute-rich environments 3INST None Medium (3 ops) Better Best quality/performance trade-off HYB Low Low-Medium Best Production deployments requiring high accuracy"},{"location":"1_decoding_methods/#implementation-considerations","title":"Implementation Considerations","text":"<p>When implementing QTIP decoding, consider these factors:</p> <ol> <li>Hardware Target: Different hardware may favor different decoding strategies:</li> <li>GPUs: HYB or 3INST leverage parallel compute well</li> <li>CPUs: 1MAD can be very efficient with SIMD</li> <li> <p>Mobile: LUT for small models, 1MAD for larger ones</p> </li> <li> <p>Memory Budget: If memory is severely constrained, prefer compute-based methods (1MAD, 3INST)</p> </li> <li> <p>Accuracy Requirements: For highest quality, use HYB with an optimized LUT</p> </li> <li> <p>Model Size: The impact of decoding method grows with model size</p> </li> </ol>"},{"location":"1_qtip_codebase/","title":"QTIP codebase","text":"<p>This document provides a structured overview of the QTIP implementation and its decoding, performance, and integration details.</p>"},{"location":"1_qtip_codebase/#1-core-components-and-methods","title":"1. Core Components and Methods","text":""},{"location":"1_qtip_codebase/#11-bitshiftcodebook","title":"1.1 BitshiftCodebook","text":"<p>Location: <code>bitshift.py</code></p> <p>This module handles all aspects of quantization, trellis management, and decoding logic.</p> <p>Key Methods: - <code>recons_lut()</code>   Reconstructs the lookup table based on the current decoding mode.</p> <ul> <li> <p><code>recons(encoded)</code>   Reconstructs floating-point values from encoded quantized indices.</p> </li> <li> <p><code>update(cost, thing)</code>   Updates the trellis state and cumulative cost during Viterbi search.</p> </li> <li> <p><code>viterbi(X, overlap=None)</code>   Performs Viterbi decoding to find the optimal quantization path.</p> </li> <li> <p><code>quantize_seq(X, overlap=None)</code>   Quantizes a sequence of input weights using trellis-structured optimization.</p> </li> <li> <p><code>quantize(X)</code>   Main method to quantize an input block.</p> </li> <li> <p><code>pack_trellis(trellis)</code>   Compresses the trellis into a bit-packed format.</p> </li> <li> <p><code>unpack_trellis(packed, T)</code>   Decompresses the packed trellis back to state sequences.</p> </li> </ul>"},{"location":"1_qtip_codebase/#12-bitshiftlinear","title":"1.2 BitshiftLinear","text":"<p>Location: <code>bitshift.py</code></p> <p>This class defines a linear layer that supports fast matrix multiplication using compressed weights.</p> <p>Key Methods: - <code>get_hatW(unpacked_trellis, m, n)</code>   Reconstructs the full weight matrix from trellis codes.</p> <ul> <li> <p><code>get_hatW_kernel(trellis, m, n)</code>   CUDA kernel version of weight reconstruction.</p> </li> <li> <p><code>cache_hatW(...)</code>   Caches reconstructed weight matrices to avoid redundant decoding during inference.</p> </li> <li> <p><code>forward(...)</code>   Executes the forward pass using trellis-decoded weights. Supports distributed and low-level kernel options.</p> </li> </ul>"},{"location":"1_qtip_codebase/#13-quantizedlinear","title":"1.3 QuantizedLinear","text":"<p>Location: <code>bitshift.py</code></p> <p>Defines a wrapper around <code>BitshiftLinear</code> with support for gradient checkpointing.</p> <p>Key Methods: - <code>no_ckpt_forward(input)</code>   Forward pass without checkpointing (faster inference).</p> <ul> <li><code>forward(input)</code>   Main forward pass. Supports gradient checkpointing for memory efficiency.</li> </ul>"},{"location":"1_qtip_codebase/#2-decoding-modes","title":"2. Decoding Modes","text":"<p>QTIP supports multiple decoding modes, each balancing speed, accuracy, and memory differently.</p>"},{"location":"1_qtip_codebase/#21-available-modes","title":"2.1 Available Modes","text":"<ul> <li> <p><code>'lut'</code>   Basic lookup table decoding; simple and fast.</p> </li> <li> <p><code>'1mad'</code>   Uses a single multiply-add operation to approximate a Gaussian.</p> </li> <li> <p><code>'2mad'</code>   Uses two MADs for better approximation.</p> </li> <li> <p><code>'3inst'</code>   Uses three instructions (LCG + bitwise) for accurate decoding.</p> </li> <li> <p><code>'quantlut'</code>   Decodes using a compact quantized lookup table.</p> </li> <li> <p><code>'quantlut_sym'</code>   Symmetric version of quantized lookup for best accuracy.</p> </li> </ul>"},{"location":"1_qtip_codebase/#3-performance-optimization","title":"3. Performance Optimization","text":""},{"location":"1_qtip_codebase/#31-cuda-implementation","title":"3.1 CUDA Implementation","text":"<p>QTIP includes CUDA kernels for: - Bitshift trellis decoding - Matrix multiplication using <code>BitshiftLinearKernelAG</code> - Custom ops via PyTorch extensions</p>"},{"location":"1_qtip_codebase/#32-memory-optimization-techniques","title":"3.2 Memory Optimization Techniques","text":"<p>Trellis Packing - <code>pack_trellis()</code>: Packs trellis into a compact bit format. - <code>unpack_trellis()</code>: Reconstructs full trellis from packed form.</p> <p>Weight Caching - <code>cache_hatW(...)</code>: Caches decoded weights to avoid recomputation during repeated inference.</p>"},{"location":"1_qtip_codebase/#4-best-practices","title":"4. Best Practices","text":""},{"location":"1_qtip_codebase/#parameter-selection","title":"Parameter Selection","text":"Parameter Recommendation <code>L</code> (trellis window) Use 16 (default) <code>K</code> (bits per weight) Use 2\u20134 for good compression <code>V</code> (group size) Use 2 for most cases <code>tlut_bits</code> Should match <code>L</code> in <code>lut</code> mode"},{"location":"1_qtip_codebase/#mode-selection","title":"Mode Selection","text":"Use Case Recommended Mode Highest accuracy <code>quantlut_sym</code> Fastest decoding <code>1mad</code> or <code>2mad</code> Simplicity <code>lut</code>"},{"location":"1_qtip_codebase/#block-size-selection","title":"Block Size Selection","text":"<ul> <li><code>td_x</code>, <code>td_y</code> should be powers of 2</li> <li>Common sizes: 16, 32, 64</li> <li>Larger blocks improve compression but increase computation</li> </ul>"},{"location":"1_qtip_codebase/#5-important-notes","title":"5. Important Notes","text":""},{"location":"1_qtip_codebase/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Different decode modes impact latency and precision</li> <li>Use CUDA kernels for best speed</li> <li>Block sizes affect cache efficiency</li> </ul>"},{"location":"1_qtip_codebase/#accuracy-considerations","title":"Accuracy Considerations","text":"<ul> <li>Quantization affects model accuracy</li> <li>Post-quantization fine-tuning may be necessary</li> <li>Mixed-precision training and inference can help recover accuracy</li> </ul>"},{"location":"1_qtip_codebase/#citation","title":"Citation","text":"<p>@inproceedings{tseng2024qtip,     title={{QTIP}: Quantization with Trellises and Incoherence Processing},     author={Albert Tseng and Qingyao Sun and David Hou and Christopher De Sa},     booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},     year={2024} }</p>"},{"location":"1_trellis_quantization/","title":"Trellis Quantization (TCQ)","text":"<p>Trellis-Coded Quantization (TCQ) is a structured quantization technique that compresses a sequence of values by finding an optimal path through a constrained state machine known as a trellis. Unlike scalar quantization, which treats each value independently, TCQ jointly optimizes the quantization of a sequence to minimize total distortion.</p> <p>This method is especially effective in compressing neural network weight matrices, where block-level patterns can be exploited for better accuracy and compression.</p>"},{"location":"1_trellis_quantization/#1-motivation","title":"1. Motivation","text":"<p>Traditional quantization, such as uniform scalar quantization, minimizes local error by rounding each value independently. However, this can lead to suboptimal results when the data has structure or correlation, as in neural network weights.</p> <p>TCQ addresses this by encoding an entire block or sequence of values together, enforcing path constraints via a trellis structure. The global sequence-level optimization leads to significantly lower average distortion.</p>"},{"location":"1_trellis_quantization/#2-the-structure-of-a-trellis","title":"2. The Structure of a Trellis","text":"<p>A trellis consists of:</p> <ul> <li>States: Each representing a configuration at a timestep</li> <li>Transitions: Connections between states allowed by a fixed rule</li> <li>Output values: Each transition emits a quantized value</li> </ul> <p>Each input value is not assigned a quantization index independently. Instead, the entire sequence is represented as a path in the trellis, and the quantized values are determined by that path.</p>"},{"location":"1_trellis_quantization/#3-quantization-as-a-path-finding-problem","title":"3. Quantization as a Path-Finding Problem","text":"<p>Given: - A quantization codebook: e.g., <code>C = [-1.0, -0.5, 0.0, 0.5, 1.0]</code> - A sequence of input values: e.g., <code>[0.4, -0.7, 0.1, 0.3]</code> - A state transition rule: e.g., each state can move to 2 of the next layer\u2019s states</p> <p>Goal: Find the path through the trellis that results in quantized values minimizing the total squared error to the input.</p>"},{"location":"1_trellis_quantization/#4-dynamic-programming-viterbi-algorithm","title":"4. Dynamic Programming (Viterbi Algorithm)","text":"<p>The Viterbi algorithm is used to efficiently find the minimum-error path:</p> <ol> <li>At each time step <code>t</code>, for each possible state <code>s</code>:</li> <li>Compute the minimum cumulative cost to reach that state from the previous layer</li> <li> <p>Store the corresponding backpointer</p> </li> <li> <p>At the final time step, select the state with the lowest total cost</p> </li> <li> <p>Backtrack from the final state to reconstruct the optimal path</p> </li> </ol>"},{"location":"1_trellis_quantization/#5-example","title":"5. Example","text":"<p>Suppose:</p> <ul> <li>Codebook: <code>[-1, 0, 1]</code></li> <li>Input sequence: <code>[0.2, -0.4, 0.5]</code></li> <li>Trellis: Each state has 2 allowed transitions to the next layer</li> </ul> <p>Rather than: Scalar Quantization \u2192 [0, -0, 1] Total Error \u2248 (0.2-0)^2 + (-0.4-0)^2 + (0.5-1)^2 \u2248 0.04 + 0.16 + 0.25 = 0.45</p> <p>TCQ would try sequences like: Path A: [0, -1, 1] \u2192 Error = (0.2-0)^2 + (-0.4+1)^2 + (0.5-1)^2 = 0.04 + 0.36 + 0.25 = 0.65 Path B: [1, 0, 0] \u2192 Error = (0.2-1)^2 + (-0.4-0)^2 + (0.5-0)^2 = 0.64 + 0.16 + 0.25 = 1.05 Path C: [0, 0, 1] \u2192 Error = 0.04 + 0.16 + 0.25 = 0.45 \u2714 (same as scalar)</p> <p>Then Viterbi will choose the best path with valid state transitions and minimal error.</p>"},{"location":"1_trellis_quantization/#6-encoding-how-tcq-paths-are-represented","title":"6. Encoding: How TCQ Paths Are Represented","text":"<p>Each path is encoded as a binary stream representing:</p> <ul> <li>The initial state</li> <li>A sequence of branch indices (e.g., which of the K possible branches was taken at each step)</li> </ul> <p>This allows compression of N quantized values into fewer bits than scalar quantization.</p> <p>Example: With K=2 branches and N=8 values, only log2(K^N) = N bits are needed.</p>"},{"location":"1_trellis_quantization/#7-bitshift-trellis-in-qtip","title":"7. Bitshift Trellis in QTIP","text":"<p>QTIP uses a specialized version of TCQ called the Bitshift Trellis, where:</p> <ul> <li>Each state is represented by an L-bit integer</li> <li>Each step:</li> <li>Shifts the current state left by 1 bit</li> <li>Appends a new input bit</li> <li>Masks to retain only L bits</li> </ul> <p>This efficient structure allows for extremely fast computation and compact code representation.</p> <p>```text Example (L = 12):</p> <p>Current state: 011011000001 Input bit: 1 New state: (011011000001 &lt;&lt; 1) | 1 = 110110000011</p>"},{"location":"environment_setup/","title":"environment setup","text":"<p>a# vLLM + QTIP Development Environment Setup (via Docker)</p> <p>This document outlines the steps for setting up a vLLM development environment with QTIP integration using Docker. The target platform is WSL2 on Windows, with CUDA-enabled GPU support.</p>"},{"location":"environment_setup/#environment-overview","title":"Environment Overview","text":"Component Details Platform Windows 11 with WSL2 (Ubuntu) GPU NVIDIA RTX (CUDA 12.8) Container Runtime Docker Desktop with WSL2 integration Build Target vllm-dev image with QTIP Python fallback code"},{"location":"environment_setup/#preparation","title":"Preparation","text":""},{"location":"environment_setup/#enable-docker-wsl2-integration","title":"Enable Docker WSL2 Integration","text":"<p>Ensure Docker Desktop is installed and WSL2 integration is enabled for the desired distro (e.g., Ubuntu 20.04).</p>"},{"location":"environment_setup/#docker-image-build","title":"Docker Image Build","text":""},{"location":"environment_setup/#1-build-the-dev-image","title":"1. Build the Dev Image","text":"<pre><code>DOCKER_BUILDKIT=1 docker build . \\\n  -f vllm/docker/Dockerfile \\\n  --target dev \\\n  -t vllm-dev\n</code></pre> <p>This command builds the dev stage from <code>vllm/docker/Dockerfile</code>, creating an image tagged <code>vllm-dev</code>.</p>"},{"location":"environment_setup/#explanation","title":"Explanation","text":"<pre><code>DOCKER_BUILDKIT=1 docker build . \\\n  --file docker/Dockerfile \\\n  --target dev \\\n  --tag vllm-dev\n</code></pre> <ul> <li><code>DOCKER_BUILDKIT=1</code>: Enables the BuildKit backend for improved performance and caching.</li> <li><code>--file</code>: Specifies the path to the Dockerfile.</li> <li><code>--target dev</code>: Builds only up to the dev stage.</li> <li><code>--tag vllm-dev</code>: Names the output image vllm-dev.</li> </ul> <p>Note: The first build may take 20+ minutes due to dependency compilation.</p>"},{"location":"environment_setup/#running-the-container","title":"Running the Container","text":"<p>After the image is built:</p> <pre><code>docker run -it --gpus all --rm --ipc=host \\\n  -v $(pwd):/workspace \\\n  vllm-dev \\\n  /bin/bash\n</code></pre>"},{"location":"environment_setup/#explanation_1","title":"Explanation","text":"<ul> <li><code>-it</code>: Launches an interactive terminal.</li> <li><code>--gpus all</code>: Enables GPU access inside the container.</li> <li><code>--ipc=host</code>: Shares host memory (important for PyTorch multiprocessing).</li> <li><code>--rm</code>: Automatically removes the container upon exit.</li> <li><code>-v $(pwd):/workspace</code>: Mounts the current host directory to /workspace inside the container.</li> <li><code>/bin/bash</code>: Starts a bash shell.</li> </ul>"},{"location":"environment_setup/#dockerfile-changes-i-made","title":"Dockerfile Changes I Made","text":""},{"location":"environment_setup/#added-libnuma-dev-to-fix-fastsafetensors-build","title":"Added libnuma-dev to fix fastsafetensors build","text":"<pre><code>FROM base as dev\nRUN apt-get update &amp;&amp; apt-get install -y libnuma-dev\n</code></pre>"},{"location":"environment_setup/#fixed-cuda-install-failures-in-base-stage","title":"Fixed CUDA install failures in base stage","text":"<p>Original line (that failed with wheel metadata errors):</p> <pre><code>RUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu128\n</code></pre> <p>Replaced with standard pip:</p> <pre><code>RUN python3 -m pip install --upgrade pip &amp;&amp; \\\n    python3 -m pip install -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu128\n</code></pre>"},{"location":"environment_setup/#summary","title":"Summary","text":"<p>This setup allows you to:</p> <ul> <li>Build vLLM with CUDA 12.8+ toolchain</li> <li>Develop and test QTIP inference modules in isolation</li> <li>Avoid polluting the host Python environment</li> </ul> <p>All development is done inside <code>/workspace</code>, and changes are instantly visible on the Windows side via volume mounting.</p>"},{"location":"qtip_overview/","title":"QTIP: Advanced LLM Quantization","text":""},{"location":"qtip_overview/#overview","title":"Overview","text":"<p>QTIP (Quantization with Trellises and Incoherence Processing) is a state-of-the-art post-training quantization method designed to compress large language models (LLMs) efficiently. It combines incoherence processing with trellis-coded quantization (TCQ) to achieve high compression rates without significant loss in model accuracy.</p>"},{"location":"qtip_overview/#key-components","title":"Key Components","text":""},{"location":"qtip_overview/#incoherence-processing","title":"Incoherence Processing","text":"<ul> <li>Employs a random Hadamard transform to decorrelate weight matrices</li> <li>Makes weights resemble independent and identically distributed (i.i.d.) Gaussian distributions</li> <li>Standardizes weight distribution for more effective quantization</li> </ul>"},{"location":"qtip_overview/#trellis-coded-quantization-tcq","title":"Trellis-Coded Quantization (TCQ)","text":"<ul> <li>Offers linear scalability unlike traditional vector quantization (which suffers from exponential complexity)</li> <li>Utilizes a \"bitshift trellis\" structure</li> <li>Introduces compute-based codes that trade memory usage for computational efficiency</li> <li>Enables fast decoding of quantized weights</li> </ul>"},{"location":"qtip_overview/#performance-highlights","title":"Performance Highlights","text":"<ul> <li>Compression Efficiency: Achieves near-optimal distortion levels across various distributions, significantly improving upon previous methods like QuIP#</li> <li>Inference Speed: Reduces model size through effective quantization, enabling faster inference (particularly in memory-bound scenarios common in LLM deployments)</li> <li>Scalability: Linear cost of TCQ in quantization dimensions allows QTIP to scale effectively to ultra-high-dimensional quantization tasks</li> </ul>"},{"location":"qtip_overview/#practical-applications","title":"Practical Applications","text":"<p>QTIP has been applied to models such as Llama 3.1 405B Instruct, demonstrating its effectiveness in real-world scenarios. The method's ability to maintain model performance while reducing size and improving inference speed makes it a valuable tool for deploying LLMs in resource-constrained environments.</p>"},{"location":"qtip_overview/#technical-implementation","title":"Technical Implementation","text":"<p>The implementation of QTIP involves several key steps:</p> <ol> <li>Preprocessing: Application of the Hadamard transform to decorrelate weight matrices</li> <li>Trellis Construction: Building an efficient bitshift trellis structure</li> <li>Quantization: Mapping weights to discrete values using the trellis structure</li> <li>Reconstruction: Efficiently decoding quantized weights during inference</li> </ol>"},{"location":"qtip_overview/#advantages-over-previous-methods","title":"Advantages Over Previous Methods","text":"Feature QTIP Traditional Methods Complexity Linear with dimensions Often exponential Distortion Near-optimal Suboptimal Memory Usage Efficient compute-memory tradeoff Often memory-intensive Scalability Excellent for ultra-high dimensions Limited by complexity"},{"location":"qtip_overview/#conclusion","title":"Conclusion","text":"<p>QTIP represents a significant advancement in LLM quantization technology, offering superior compression with minimal performance degradation. Its innovative approach to handling high-dimensional weight matrices makes it particularly valuable for deploying large models in resource-constrained environments.</p> <p>For more detailed information, refer to the original blog post: Even Better, Even Faster Quantized LLMs with QTIP.</p>"},{"location":"vllm_integration_steps/","title":"Integrating QTIP into vLLM \u2014 Step-by-Step Guide","text":"<p>This guide walks through the exact steps to integrate the QTIP quantization method into vLLM's inference engine. By following this, you will be able to run QTIP-compressed models directly with vLLM's fast and scalable infrastructure.</p>"},{"location":"vllm_integration_steps/#step-1-prepare-quantize_configjson","title":"Step 1: Prepare <code>quantize_config.json</code>","text":"<p>Create a QTIP configuration file. Example:</p> <pre><code>{\n  \"quant_method\": \"qtip\",\n  \"L\": 16,\n  \"K\": 2,\n  \"V\": 2,\n  \"tlut_bits\": 16,\n  \"decode_mode\": \"1mad\",\n  \"quantlut_path\": \"checkpoints/llama/qtip/quantlut.pt\"\n}\n</code></pre> <p>This config must be placed in the model folder, or passed via CLI when launching vLLM.</p>"},{"location":"vllm_integration_steps/#step-2-define-qtipconfig-in-quantization_configpy","title":"Step 2: Define QTIPConfig (in quantization_config.py)","text":"<p>Create a new class to parse the QTIP config:</p> <pre><code>from typing import Any, Optional\nimport torch\nfrom vllm.model_executor.layers.quantization.base_config import QuantizationConfig\nfrom vllm.model_executor.layers.quantization import QuantizationMethods\n\nclass QTIPConfig(QuantizationConfig):\n    \"\"\"Config class for QTIP quantization.\"\"\"\n\n    def __init__(self, L: int, K: int, V: int, tlut_bits: int = 16,\n                 decode_mode: str = \"1mad\", quantlut_path: Optional[str] = None):\n        super().__init__()\n        self.L = L  # Number of codebooks\n        self.K = K  # Number of centroids per codebook\n        self.V = V  # Number of vectors per centroid\n        self.tlut_bits = tlut_bits  # Bits for lookup table\n        self.decode_mode = decode_mode  # Decoding mode: \"1mad\", \"3inst\", or \"quantlut\"\n        self.quantlut_path = quantlut_path  # Path to quantization lookup table\n\n    @classmethod\n    def get_name(cls) -&gt; QuantizationMethods:\n        return \"qtip\"\n\n    @classmethod \n    def get_supported_act_dtypes(cls) -&gt; list[torch.dtype]:\n        return [torch.float16, torch.bfloat16]\n\n    @classmethod\n    def get_min_capability(cls) -&gt; int:\n        return 70  # Minimum CUDA capability required\n\n    @classmethod\n    def get_config_filenames(cls) -&gt; list[str]:\n        return [\"quantize_config.json\"]\n\n    @classmethod\n    def from_config(cls, config: dict[str, Any]) -&gt; \"QTIPConfig\":\n        L = cls.get_from_keys(config, [\"L\"])\n        K = cls.get_from_keys(config, [\"K\"]) \n        V = cls.get_from_keys(config, [\"V\"])\n        tlut_bits = cls.get_from_keys_or(config, [\"tlut_bits\"], 16)\n        decode_mode = cls.get_from_keys_or(config, [\"decode_mode\"], \"1mad\")\n        quantlut_path = cls.get_from_keys_or(config, [\"quantlut_path\"], None)\n\n        return cls(L=L, K=K, V=V, tlut_bits=tlut_bits,\n                  decode_mode=decode_mode, quantlut_path=quantlut_path)\n\n    def get_quant_method(self, layer: torch.nn.Module,\n                        prefix: str) -&gt; Optional[\"QTIPLinearMethod\"]:\n        if isinstance(layer, LinearBase):\n            return QTIPLinearMethod(self)\n        return None\n</code></pre>"},{"location":"vllm_integration_steps/#step-3-define-qtiplinearmethod-in-methodpy","title":"Step 3: Define QTIPLinearMethod (in method.py)","text":"<p>Implement a new quantization method:</p> <pre><code>from typing import Optional\nimport torch\nfrom vllm.model_executor.layers.linear import LinearBase, LinearMethodBase\nfrom vllm.model_executor.layers.quantization.base_config import QuantizeMethodBase\nfrom vllm.model_executor.utils import set_weight_attrs\n\nclass QTIPLinearMethod(QuantizeMethodBase):\n    \"\"\"Linear method for QTIP quantization.\"\"\"\n\n    def __init__(self, config: QTIPConfig):\n        self.config = config\n\n    def create_weights(self, layer: torch.nn.Module,\n                      input_size_per_partition: int,\n                      output_partition_sizes: list[int],\n                      input_size: int,\n                      output_size: int,\n                      params_dtype: torch.dtype,\n                      **extra_weight_attrs):\n        # Create compressed weight storage\n        weight = torch.nn.Parameter(\n            torch.empty(sum(output_partition_sizes),\n                       input_size_per_partition,\n                       dtype=params_dtype),\n            requires_grad=False)\n\n        # Initialize codebook\n        cb = BitshiftCodebook(self.config)\n        cb.quantize(weight)\n        layer.qtip_cb = cb\n\n        set_weight_attrs(weight, {\"input_dim\": 1, \"output_dim\": 0})\n        layer.register_parameter(\"weight\", weight)\n        set_weight_attrs(weight, extra_weight_attrs)\n\n    def apply(self, layer: torch.nn.Module,\n              x: torch.Tensor,\n              bias: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        # Decode weights if not cached\n        if not hasattr(layer, \"decoded_weight\"):\n            layer.decoded_weight = layer.qtip_cb.recons()\n\n        # Perform matrix multiplication\n        out = torch.matmul(x, layer.decoded_weight.t())\n        if bias is not None:\n            out += bias\n        return out\n\n    def process_weights_after_loading(self, layer: torch.nn.Module) -&gt; None:\n        # Load quantization lookup table if specified\n        if self.config.quantlut_path:\n            layer.qtip_cb.load_quantlut(self.config.quantlut_path)\n</code></pre>"},{"location":"vllm_integration_steps/#step-4-implement-quantizedlinear-in-layersquantized_linearpy","title":"Step 4: Implement QuantizedLinear (in layers/quantized_linear.py)","text":"<p>This wrapper replaces the original nn.Linear:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom typing import Optional\n\nclass QuantizedLinear(nn.Module):\n    \"\"\"Quantized linear layer using QTIP method.\"\"\"\n\n    def __init__(self, original_layer: nn.Linear, config: QTIPConfig):\n        super().__init__()\n        self.qtip_cb = None\n        self.bias = original_layer.bias\n        self.config = config\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Decode weights if not cached\n        if not hasattr(self, \"decoded_weight\"):\n            if self.config.decode_mode == \"1mad\":\n                self.decoded_weight = decode_1mad(self.qtip_cb.encoded, self.config)\n            elif self.config.decode_mode == \"3inst\":\n                self.decoded_weight = decode_3inst(self.qtip_cb.encoded, self.config)\n            elif self.config.decode_mode == \"quantlut\":\n                self.decoded_weight = lookup_decode(self.qtip_cb.encoded, self.config)\n            else:\n                raise ValueError(f\"Unsupported decode mode: {self.config.decode_mode}\")\n\n        # Perform matrix multiplication\n        out = torch.matmul(x, self.decoded_weight.t())\n        if self.bias is not None:\n            out += self.bias\n        return out\n</code></pre>"},{"location":"vllm_integration_steps/#step-5-add-python-fallback-decode-in-bitshift_codebookpy","title":"Step 5: Add Python Fallback Decode (in bitshift_codebook.py)","text":"<p>Make sure your BitshiftCodebook implements:</p> <pre><code>import torch\nfrom typing import Optional\n\ndef decode_1mad(encoded: torch.Tensor, config: QTIPConfig) -&gt; torch.Tensor:\n    \"\"\"Decode using 1-MAD (Multiply-Add) method.\"\"\"\n    # Implementation of 1-MAD decoding\n    # This is a simplified version - actual implementation would be more complex\n    decoded = torch.zeros_like(encoded)\n    for i in range(config.L):\n        codebook = encoded[i]\n        decoded += codebook\n    return decoded\n\ndef decode_3inst(encoded: torch.Tensor, config: QTIPConfig) -&gt; torch.Tensor:\n    \"\"\"Decode using 3-instruction method.\"\"\"\n    # Implementation of 3-instruction decoding\n    # This is a simplified version - actual implementation would be more complex\n    decoded = torch.zeros_like(encoded)\n    for i in range(config.L):\n        codebook = encoded[i]\n        decoded += codebook\n    return decoded\n\ndef lookup_decode(encoded: torch.Tensor, config: QTIPConfig) -&gt; torch.Tensor:\n    \"\"\"Decode using lookup table method.\"\"\"\n    # Implementation of lookup table decoding\n    # This is a simplified version - actual implementation would be more complex\n    if not hasattr(config, \"quantlut\"):\n        config.load_quantlut(config.quantlut_path)\n    return config.quantlut[encoded]\n\nclass BitshiftCodebook:\n    \"\"\"Codebook for QTIP quantization.\"\"\"\n\n    def __init__(self, config: QTIPConfig):\n        self.config = config\n        self.encoded = None\n\n    def quantize(self, weight: torch.Tensor) -&gt; None:\n        \"\"\"Quantize weights using QTIP method.\"\"\"\n        # Implementation of quantization\n        # This is a simplified version - actual implementation would be more complex\n        self.encoded = weight.clone()\n\n    def recons(self) -&gt; torch.Tensor:\n        \"\"\"Reconstruct weights from codebook.\"\"\"\n        if self.config.decode_mode == \"1mad\":\n            return decode_1mad(self.encoded, self.config)\n        elif self.config.decode_mode == \"3inst\":\n            return decode_3inst(self.encoded, self.config)\n        elif self.config.decode_mode == \"quantlut\":\n            return lookup_decode(self.encoded, self.config)\n        else:\n            raise ValueError(f\"Unsupported decode mode: {self.config.decode_mode}\")\n\n    def load_quantlut(self, path: str) -&gt; None:\n        \"\"\"Load quantization lookup table.\"\"\"\n        self.quantlut = torch.load(path)\n</code></pre>"},{"location":"vllm_integration_steps/#step-6-optional-register-custom-cuda-kernel-in-_custom_ops","title":"Step 6: Optional \u2014 Register Custom CUDA Kernel (in _custom_ops/)","text":"<p>If you have a kernel version of BitshiftLinear (e.g., bitshift_linear_kernel.cu), register it as:</p> <pre><code>// bitshift_linear_kernel.cu\n#include &lt;torch/extension.h&gt;\n#include &lt;cuda.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n// CUDA kernel for QTIP linear layer\n__global__ void bitshift_gemm_kernel(\n    const float* input,\n    const float* weight,\n    float* output,\n    int m, int n, int k) {\n    // Implementation of CUDA kernel\n    // This is a simplified version - actual implementation would be more complex\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; m * n) {\n        int i = idx / n;\n        int j = idx % n;\n        float sum = 0.0f;\n        for (int kk = 0; kk &lt; k; kk++) {\n            sum += input[i * k + kk] * weight[kk * n + j];\n        }\n        output[idx] = sum;\n    }\n}\n\ntorch::Tensor bitshift_gemm(\n    torch::Tensor input,\n    torch::Tensor weight,\n    torch::Tensor bias) {\n\n    auto m = input.size(0);\n    auto k = input.size(1);\n    auto n = weight.size(0);\n\n    auto output = torch::empty({m, n}, input.options());\n\n    dim3 block(256);\n    dim3 grid((m * n + block.x - 1) / block.x);\n\n    bitshift_gemm_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(\n        input.data_ptr&lt;float&gt;(),\n        weight.data_ptr&lt;float&gt;(),\n        output.data_ptr&lt;float&gt;(),\n        m, n, k);\n\n    if (bias.defined()) {\n        output += bias;\n    }\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"bitshift_gemm\", &amp;bitshift_gemm, \"QTIP linear layer (CUDA)\");\n}\n</code></pre> <p>Also create a CMake entry to build the op:</p> <pre><code>find_package(CUDA REQUIRED)\nfind_package(Torch REQUIRED)\n\nadd_library(bitshift_linear SHARED\n    bitshift_linear_kernel.cu\n)\n\ntarget_link_libraries(bitshift_linear\n    ${TORCH_LIBRARIES}\n    ${CUDA_LIBRARIES}\n)\n\nset_target_properties(bitshift_linear PROPERTIES\n    CUDA_SEPARABLE_COMPILATION ON\n)\n</code></pre>"},{"location":"vllm_integration_steps/#step-7-test-the-integration","title":"Step 7: Test the Integration","text":"<p>\u2705 Run with <code>python -m vllm.entrypoints.openai.api_server --quantize_config quantize_config.json</code></p> <p>\u2705 Load a model compressed with QTIP</p> <p>\u2705 Send a sample prompt</p> <p>\u2705 Verify decode speed and correctness</p> <p>This implementation provides basic QTIP quantization functionality, including:</p> <ul> <li>Support for multiple decoding modes (1-MAD, 3-instruction, lookup table)</li> <li>Configurable quantization parameters (L, K, V, bits, etc.)</li> <li>CUDA acceleration support</li> <li>Integration with vLLM's existing quantization framework</li> </ul> <p>You can further optimize and extend this implementation based on your specific needs.</p>"},{"location":"vllm_overview/","title":"vLLM Overview","text":"<p><code>vLLM</code> is a high-performance, open-source inference engine for large language models (LLMs). It is designed to maximize throughput, minimize latency, and reduce memory usage for serving models like LLaMA, GPT-J, Falcon, and others in production and research environments.</p> <p>vLLM combines system-level optimizations with deep model-aware scheduling to enable fast, parallel, and memory-efficient inference.</p>"},{"location":"vllm_overview/#key-features-of-vllm","title":"Key Features of vLLM","text":""},{"location":"vllm_overview/#1-pagedattention","title":"1. PagedAttention","text":"<p>vLLM introduces PagedAttention, a novel memory layout that: - Enables efficient dynamic batching - Minimizes memory fragmentation - Reduces the cost of KV cache management - Supports partial KV eviction and reuse</p> <p>This makes vLLM particularly suitable for serving many simultaneous requests with varying sequence lengths.</p>"},{"location":"vllm_overview/#2-kernel-fusion-and-gpu-efficiency","title":"2. Kernel Fusion and GPU Efficiency","text":"<p>vLLM implements: - Fused multi-head attention - Optimized matmul and softmax kernels - Efficient quantized inference support</p> <p>These optimizations allow it to fully utilize modern GPUs with high compute-to-memory ratios (e.g., A100, H100).</p>"},{"location":"vllm_overview/#3-continuous-batching-scheduler","title":"3. Continuous Batching Scheduler","text":"<p>Unlike frameworks that batch requests synchronously, vLLM supports: - Asynchronous token streaming - Request prioritization - Fine-grained batching</p> <p>This allows it to achieve sub-50ms latency even with many concurrent users.</p>"},{"location":"vllm_overview/#model-support-and-quantization","title":"Model Support and Quantization","text":"<p>vLLM supports models trained in Hugging Face or PyTorch format, including: - LLaMA / LLaMA 2 / LLaMA 3 - Falcon - OPT - GPT-J / GPT-NeoX - MPT</p> <p>It also supports: - GPTQ quantized models - AWQ quantized models - LoRA adapters</p> <p>However, advanced quantization methods like QTIP are not natively supported, which is where this project contributes.</p>"},{"location":"vllm_overview/#vllm-structure-overview","title":"vLLM Structure Overview","text":"Component Description <code>model_executor/</code> Loads models, processes forward pass <code>model_executor/layers/</code> Defines custom Linear, Attention, Norm layers <code>model_executor/quantization</code> Handles quantization wrappers (e.g., GPTQ) <code>engine/</code> Handles request scheduling and batching <code>_custom_ops/</code> CUDA kernels and fused ops <code>quantization_config.json</code> Defines external quantization configuration"},{"location":"vllm_overview/#where-qtip-fits-in","title":"Where QTIP Fits In","text":"<p>To integrate QTIP into vLLM, we must: - Define a new quantization method class (<code>QTIPLinearMethod</code>) - Replace <code>torch.nn.Linear</code> with a custom <code>BitshiftLinear</code> or <code>QuantizedLinear</code> class - Hook into the loading pipeline via <code>load_quant_config</code> and <code>QuantizationConfig</code> - Optionally implement a CUDA kernel path or use Python fallback logic</p> <p>This enables vLLM to serve QTIP-compressed models with minimal accuracy loss and maximum runtime efficiency.</p>"},{"location":"vllm_overview/#summary","title":"Summary","text":"<p>vLLM is an inference-first engine built for LLMs at scale. Its modular architecture, performance-focused kernel design, and flexible quantization interface make it an ideal target for integrating cutting-edge compression techniques like QTIP.</p>"},{"location":"vllm_quantization/","title":"Understanding Quantization in vLLM","text":"<p>This section explains how quantization is implemented in vLLM, focusing on the architecture of weight-only post-training quantization (PTQ). We analyze how methods like GPTQ and AWQ are supported, and how QTIP can leverage the same modularity to integrate efficiently.</p>"},{"location":"vllm_quantization/#1-quantization-architecture-in-vllm","title":"1. Quantization Architecture in vLLM","text":"<p>vLLM adopts a plugin-style quantization system. Each quantization method is defined by:</p> <ul> <li>A configuration class (<code>QuantizationConfig</code>)</li> <li>A method class (<code>QuantizationMethod</code>)</li> <li>A custom layer implementation (<code>QuantizedLinear</code>, etc.)</li> </ul> <p>This system is built to allow different quantization schemes to be swapped in and configured without changing model code.</p>"},{"location":"vllm_quantization/#11-configuration-entry-point-quantize_configjson","title":"1.1 Configuration Entry Point: <code>quantize_config.json</code>","text":"<p>Users specify their quantization settings via a JSON config file. Example for GPTQ:</p> <pre><code>{\n  \"quant_method\": \"gptq\",\n  \"bits\": 4,\n  \"group_size\": 128,\n  \"desc_act\": false,\n  \"sym\": true\n}\n</code></pre> <p>This config is loaded and passed to the appropriate <code>QuantizationConfig</code> subclass.</p>"},{"location":"vllm_quantization/#12-quantizationconfig-and-method-class","title":"1.2 QuantizationConfig and Method Class","text":"<p>Every method implements two key components:</p> Component Description <code>QuantizationConfig</code> Parses config file and holds parameters <code>QuantizationMethod</code> Applies layer substitution and post-load processing <p>GPTQ Example</p> <p><code>GPTQConfig</code> parses: - bits, group_size, sym, etc.</p> <p><code>GPTQLinearMethod</code> does: - Replaces <code>nn.Linear</code> with <code>GPTQLinear</code> - Loads quantized weights from disk or memory - Registers kernel-based <code>gptq_gemm</code> for inference</p>"},{"location":"vllm_quantization/#2-layer-replacement-flow","title":"2. Layer Replacement Flow","text":""},{"location":"vllm_quantization/#step-by-step-flow","title":"Step-by-Step Flow","text":"<ol> <li><code>model_loader.py</code> reads the <code>quantize_config.json</code></li> <li>It instantiates a <code>QuantizationConfig</code> based on <code>quant_method</code></li> <li>Calls <code>.get_method()</code> to return a <code>QuantizationMethod</code> object</li> <li>The <code>apply()</code> method replaces <code>torch.nn.Linear</code> with custom logic</li> <li>The <code>process_weights_after_loading()</code> method quantizes or prepares the weights</li> </ol> <p>This structure enables QTIP to seamlessly fit into the same flow.</p> <pre><code>graph TD\n    A[Load Model] --&gt; B[Read quantize_config.json]\n    B --&gt; C[Instantiate QuantizationConfig]\n    C --&gt; D[Get QuantizationMethod]\n    D --&gt; E[Apply layer replacement]\n    E --&gt; F[Process weights]\n    F --&gt; G[Ready for inference]\n</code></pre>"},{"location":"vllm_quantization/#3-custom-layer-logic","title":"3. Custom Layer Logic","text":""},{"location":"vllm_quantization/#for-gptq","title":"For GPTQ:","text":"<p>The core linear layer becomes <code>GPTQLinear</code>, which: - Stores compressed weight groups - Applies scaling/zero-point corrections - Uses <code>gptq_gemm()</code> CUDA kernel during forward pass</p>"},{"location":"vllm_quantization/#for-qtip-this-would-become","title":"For QTIP, this would become:","text":"<p><code>QuantizedLinear</code> (a wrapper around <code>BitshiftLinear</code>) - Supports decode modes (lut, 1mad, 3inst) - Can use:   - CUDA kernel path (e.g., <code>bitshift_linear_kernel</code>)   - Python fallback path (e.g., <code>decode_compressed + torch.matmul</code>)</p>"},{"location":"vllm_quantization/#4-where-to-hook-qtip","title":"4. Where to Hook QTIP","text":"Location Purpose QTIP Integration Point <code>model_loader.py</code> Loads config Register \"qtip\" as quant method <code>quantization/quant_config.py</code> Config parsing Add <code>QTIPConfig</code> <code>quantization/method.py</code> Method interface Add <code>QTIPLinearMethod</code> <code>layers/quantized_linear.py</code> Layer definition Add <code>QuantizedLinear</code> class <code>custom_ops/bitshift_linear.cpp</code> CUDA kernel (optional) Register bitshift kernel (optional) <p>This design keeps all QTIP logic localized and modular.</p>"},{"location":"vllm_quantization/#5-implementation-example","title":"5. Implementation Example","text":"<p>Here's a simplified code example showing how the QTIP integration might look:</p> <pre><code># In quantization/quant_config.py\nclass QTIPConfig(QuantizationConfig):\n    def __init__(self, bits=4, group_size=128, decode_mode=\"lut\"):\n        self.bits = bits\n        self.group_size = group_size\n        self.decode_mode = decode_mode\n\n    def get_method(self):\n        return QTIPLinearMethod(self)\n\n\n# In quantization/method.py\nclass QTIPLinearMethod(QuantizationMethod):\n    def apply(self, model):\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Linear):\n                # Replace with QTIP version\n                qtip_layer = QuantizedLinear(\n                    module.in_features,\n                    module.out_features,\n                    self.config.bits,\n                    self.config.group_size,\n                    self.config.decode_mode\n                )\n                # ... set up replacement ...\n\n    def process_weights_after_loading(self, model):\n        # Apply incoherence processing if needed\n        # Set up bitshift trellis and compressed weights\n</code></pre>"},{"location":"vllm_quantization/#6-extending-for-dynamic-decode-mode-selection","title":"6. Extending for Dynamic Decode Mode Selection","text":"<p>One advantage of QTIP is the ability to switch decode modes at runtime. This can be implemented by:</p> <ol> <li> <p>Adding a runtime flag to the vLLM server:    <code>bash    python -m vllm.entrypoints.api_server --decode-mode=1mad</code></p> </li> <li> <p>Updating the <code>QuantizedLinear</code> implementation to support mode switching:    <code>python    class QuantizedLinear(nn.Module):        def forward(self, x):            # Check current decode mode            mode = get_current_decode_mode()            if mode == \"lut\":                return self._forward_lut(x)            elif mode == \"1mad\":                return self._forward_1mad(x)            # ...</code></p> </li> </ol> <p>This allows users to experiment with different decode modes without reloading the model.</p>"},{"location":"vllm_quantization/#7-performance-considerations","title":"7. Performance Considerations","text":"<p>When implementing QTIP in vLLM, several performance considerations should be kept in mind:</p> <ul> <li>Memory Locality: Keep the LUT tables (if used) in shared memory or L1 cache</li> <li>Kernel Fusion: Where possible, fuse the decode and matmul operations</li> <li>Parallelism: Ensure the decode methods are highly parallelizable for GPU execution</li> <li>KV Cache Interaction: Consider how quantization affects KV cache management</li> </ul> <p>By carefully integrating with vLLM's existing architecture, QTIP can deliver its compression benefits while maintaining the performance advantages of vLLM's paged attention system.</p>"},{"location":"what_is_quantization/","title":"Understanding Quantization in Deep Learning","text":""},{"location":"what_is_quantization/#what-is-quantization","title":"What is Quantization?","text":"<p>Quantization is the process of approximating continuous or high-precision values with discrete, lower-precision ones. In deep learning, it commonly refers to converting 32-bit floating-point weights and activations into lower-bit formats like int8, int4, or even binary.</p> <p>Example: Instead of representing a weight as 3.14159265 (float32), we can represent it as 3 (int8), accepting a small error in exchange for speed and memory savings.</p> <p>This transformation drastically reduces model size and computational cost\u2014enabling efficient inference on: - Mobile devices - Embedded systems - Large-scale cloud services</p>"},{"location":"what_is_quantization/#why-quantization","title":"Why Quantization?","text":"<p>Modern large language models (LLMs) contain billions of parameters. Deploying these models at scale presents several challenges:</p> Challenge Description Memory Bandwidth Loading float32 weights for each layer consumes massive bandwidth Inference Latency Matrix multiplication with float32 is slower than with int8 Hardware Limitations Many devices (smartphones, IoT boards) cannot efficiently process float32 <p>Quantization addresses these issues by reducing data precision while preserving as much model accuracy as possible, enabling real-time inference with less power and hardware cost.</p>"},{"location":"what_is_quantization/#quantization-techniques","title":"Quantization Techniques","text":""},{"location":"what_is_quantization/#from-uniform-scalar-quantization-to-scale-zeropoint","title":"From Uniform Scalar Quantization to Scale-ZeroPoint","text":""},{"location":"what_is_quantization/#uniform-scalar-quantizer-usq","title":"Uniform Scalar Quantizer (USQ)","text":"<p>The most basic quantizer maps floating-point values to fixed intervals (buckets):</p> <ol> <li>Define float value range (e.g., \u221210 to +10)</li> <li>Choose number of levels (e.g., 256 levels for 8 bits)</li> <li>Calculate bucket width = (max \u2212 min) / (levels - 1) \u2248 0.078</li> <li>Round each value to the nearest bucket center</li> </ol>"},{"location":"what_is_quantization/#scale-and-zero-point-framework","title":"Scale and Zero-Point Framework","text":"<p>To generalize the process and make it hardware-friendly, we use two parameters:</p> <ul> <li>Scale (\u03b1): the step size between quantization levels</li> <li>Zero-point (z): the integer value that corresponds to 0 in float</li> </ul> <p>Formulas: - Quantization: <code>q = round(x / scale) + zero_point</code> - Dequantization: <code>x = scale * (q - zero_point)</code></p> <p>Example: For values in range [0, 6] using 8-bit unsigned integers (0-255): - Scale = (6 - 0) / 255 = 0.0235 - Zero-point = 0 (since 0 maps to 0) - Float 3.0 \u2192 Quantized = round(3.0 / 0.0235) = 128 - Back to float: 0.0235 * (128 - 0) \u2248 3.008</p> <p>This simple method is known as affine quantization.</p>"},{"location":"what_is_quantization/#types-of-quantization","title":"Types of Quantization","text":""},{"location":"what_is_quantization/#1-value-mapping","title":"1. Value Mapping","text":"<ul> <li>Symmetric: Zero-point is 0; float 0 always maps to int 0</li> <li>Asymmetric: Zero-point can be non-zero, better for unbalanced ranges</li> </ul>"},{"location":"what_is_quantization/#2-granularity","title":"2. Granularity","text":"<ul> <li>Per-tensor: One scale/zero-point for whole weight tensor</li> <li>Per-channel: Different scale/zero-point for each output channel</li> </ul>"},{"location":"what_is_quantization/#3-timing","title":"3. Timing","text":"<ul> <li>Post-Training Quantization (PTQ): Apply quantization after model training</li> <li>Quantization-Aware Training (QAT): Simulate quantization during training</li> </ul>"},{"location":"what_is_quantization/#methods-of-quantization","title":"Methods of Quantization","text":""},{"location":"what_is_quantization/#1-uniform-quantization","title":"1. Uniform Quantization","text":"<ul> <li>Uses fixed step sizes (as explained above)</li> <li>Fast and easy to implement</li> </ul>"},{"location":"what_is_quantization/#2-non-uniform-quantization","title":"2. Non-Uniform Quantization","text":"<ul> <li>Step size varies (e.g., logarithmic)</li> <li>Can better preserve small values but needs lookup tables</li> </ul>"},{"location":"what_is_quantization/#3-weight-only-quantization","title":"3. Weight-Only Quantization","text":"<ul> <li>Only compresses model weights, not activations</li> <li>Useful when activation computation is not a bottleneck</li> </ul>"},{"location":"what_is_quantization/#4-activation-quantization","title":"4. Activation Quantization","text":"<ul> <li>Applies to intermediate layer outputs</li> <li>Reduces memory bandwidth</li> </ul>"},{"location":"what_is_quantization/#5-mixed-precision-quantization","title":"5. Mixed-Precision Quantization","text":"<ul> <li>Combines multiple bit-widths depending on sensitivity of layers</li> <li>Often used in practice (e.g., 8-bit activations, 4-bit weights)</li> </ul>"},{"location":"what_is_quantization/#applications-of-quantization","title":"Applications of Quantization","text":""},{"location":"what_is_quantization/#1-on-device-ai","title":"1. On-Device AI","text":"<p>Quantization enables efficient inference on phones, edge devices, and embedded systems.</p>"},{"location":"what_is_quantization/#2-cloud-inference-at-scale","title":"2. Cloud Inference at Scale","text":"<p>Cloud models save compute and power costs by running quantized versions.</p>"},{"location":"what_is_quantization/#3-faster-model-loading","title":"3. Faster Model Loading","text":"<p>Compressed weights reduce I/O load and load times.</p>"},{"location":"what_is_quantization/#4-custom-hardware-acceleration","title":"4. Custom Hardware Acceleration","text":"<p>Quantized models align better with AI chips like TPUs, NPUs, and Tensor Cores.</p>"},{"location":"what_is_quantization/#5-advanced-research","title":"5. Advanced Research","text":"<p>Methods like Trellis-Coded Quantization (TCQ) push boundaries of low-bit compression.</p>"}]}