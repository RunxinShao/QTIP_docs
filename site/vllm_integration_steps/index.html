
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../vllm_quantization/">
      
      
      
      <link rel="icon" href="../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>vLLM Integration Steps - QTIP Project Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#integrating-qtip-into-vllm-step-by-step-guide" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="QTIP Project Notes" class="md-header__button md-logo" aria-label="QTIP Project Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            QTIP Project Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              vLLM Integration Steps
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="QTIP Project Notes" class="md-nav__button md-logo" aria-label="QTIP Project Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    QTIP Project Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../what_is_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    what is Quantization
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Understanding QTIP paper&code in a not-so-right way
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Understanding QTIP paper&code in a not-so-right way
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qtip_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QTIP Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_trellis_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Trellis Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_decoding_methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Decoding Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_qtip_codebase/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QTIP Codebase
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Integration with vLLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Integration with vLLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../environment_setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    environment setup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vllm_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    vLLM Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vllm_quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Understanding vLLM Quantization part
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    vLLM Integration Steps
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    vLLM Integration Steps
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#step-1-prepare-quantize_configjson" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Prepare quantize_config.json
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-2-define-qtipconfig-and-qtiplinearmethod-in-vllmmodel_executorlayersquantizationqtippy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Define QTIPConfig and QTIPLinearMethod (in vllm/model_executor/layers/quantization/qtip.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-4-define-the-bitshift-codebook-in-_custom_opspy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Define the Bitshift Codebook (in _custom_ops.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-5-implement-decoding-functions-for-qtip-in-_custom_opspy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 5: Implement Decoding Functions for QTIP (in _custom_ops.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-6-implement-the-bitshift-gemm-function-in-_custom_opspy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 6: Implement the Bitshift GEMM Function (in _custom_ops.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-7-register-qtip-in-the-quantization-methods-registryin-vllmmodel_executorlayersquantizationinitpy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 7: Register QTIP in the Quantization Methods Registry(in vllm/model_executor/layers/quantization/init.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-8-test-the-integration-in-testskernelsquantizationtest_qtippy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 8: Test the Integration (in tests/kernels/quantization/test_qtip.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-8-test-the-config-in-testsquantizationtest_qtip_configpy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 8: Test the Config (in tests\quantization\test_qtip_config.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-9-testing-command-in-the-workspace-of-docker" class="md-nav__link">
    <span class="md-ellipsis">
      Step 9: Testing command (in the workspace of docker)
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#step-1-prepare-quantize_configjson" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Prepare quantize_config.json
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-2-define-qtipconfig-and-qtiplinearmethod-in-vllmmodel_executorlayersquantizationqtippy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Define QTIPConfig and QTIPLinearMethod (in vllm/model_executor/layers/quantization/qtip.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-4-define-the-bitshift-codebook-in-_custom_opspy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Define the Bitshift Codebook (in _custom_ops.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-5-implement-decoding-functions-for-qtip-in-_custom_opspy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 5: Implement Decoding Functions for QTIP (in _custom_ops.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-6-implement-the-bitshift-gemm-function-in-_custom_opspy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 6: Implement the Bitshift GEMM Function (in _custom_ops.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-7-register-qtip-in-the-quantization-methods-registryin-vllmmodel_executorlayersquantizationinitpy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 7: Register QTIP in the Quantization Methods Registry(in vllm/model_executor/layers/quantization/init.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-8-test-the-integration-in-testskernelsquantizationtest_qtippy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 8: Test the Integration (in tests/kernels/quantization/test_qtip.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-8-test-the-config-in-testsquantizationtest_qtip_configpy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 8: Test the Config (in tests\quantization\test_qtip_config.py)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-9-testing-command-in-the-workspace-of-docker" class="md-nav__link">
    <span class="md-ellipsis">
      Step 9: Testing command (in the workspace of docker)
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="integrating-qtip-into-vllm-step-by-step-guide">Integrating QTIP into vLLM — Step-by-Step Guide<a class="headerlink" href="#integrating-qtip-into-vllm-step-by-step-guide" title="Permanent link">&para;</a></h1>
<p>This guide walks through the exact steps to integrate the QTIP (Quantization with Trellis Index Packing) quantization method into vLLM's inference engine. By following this, you will be able to run QTIP-compressed models directly with vLLM's fast and scalable infrastructure.</p>
<p>current code branch here: https://github.com/RunxinShao/vllm/tree/add-qtip-inference</p>
<h2 id="step-1-prepare-quantize_configjson">Step 1: Prepare <code>quantize_config.json</code><a class="headerlink" href="#step-1-prepare-quantize_configjson" title="Permanent link">&para;</a></h2>
<p>Create a QTIP configuration file. Example:</p>
<pre><code class="language-json">{
  &quot;quant_method&quot;: &quot;qtip&quot;,
  &quot;td_x&quot;: 16,
  &quot;td_y&quot;: 16,
  &quot;L&quot;: 16,
  &quot;K&quot;: 2,
  &quot;V&quot;: 1,
  &quot;tlut_bits&quot;: 16,
  &quot;decode_mode&quot;: &quot;1mad&quot;,
  &quot;scale&quot;: 32.0
}
</code></pre>
<p>This config must be placed in the model folder, or passed via CLI when launching vLLM.</p>
<h2 id="step-2-define-qtipconfig-and-qtiplinearmethod-in-vllmmodel_executorlayersquantizationqtippy">Step 2: Define QTIPConfig and QTIPLinearMethod (in vllm/model_executor/layers/quantization/qtip.py)<a class="headerlink" href="#step-2-define-qtipconfig-and-qtiplinearmethod-in-vllmmodel_executorlayersquantizationqtippy" title="Permanent link">&para;</a></h2>
<p>Create a new class to parse the QTIP config:</p>
<pre><code class="language-python"># SPDX-License-Identifier: Apache-2.0
from typing import Optional

import torch
from torch import nn

from vllm._custom_ops import bitshift_codebook, bitshift_gemm
from vllm.model_executor.layers.linear import LinearMethodBase
from vllm.model_executor.layers.quantization.base_config import (
    QuantizationConfig)
from vllm.model_executor.parameter import PackedvLLMParameter


class QTIPConfig(QuantizationConfig):
    &quot;&quot;&quot;
    QTIP (Quantization with Trellis Index Packing) static quantization 
    configuration class
    &quot;&quot;&quot;

    def __init__(self,
                 td_x: int,
                 td_y: int,
                 L: int,
                 K: int,
                 V: int,
                 tlut_bits: int,
                 decode_mode: str,
                 scale: float = 32.0):
        self.td_x = td_x
        self.td_y = td_y
        self.L = L
        self.K = K
        self.V = V
        self.tlut_bits = tlut_bits
        self.decode_mode = decode_mode
        self.scale = scale
        # Number of elements in each block
        self.pack_factor = td_x * td_y

    def __repr__(self) -&gt; str:
        return (
            f&quot;QTIPConfig(td_x={self.td_x}, td_y={self.td_y},&quot;
            f&quot; L={self.L}, K={self.K}, V={self.V},&quot;
            f&quot; tlut_bits={self.tlut_bits}, decode_mode='{self.decode_mode}',&quot;
            f&quot; scale={self.scale})&quot;)

    @classmethod
    def get_name(cls) -&gt; str:
        return &quot;qtip&quot;

    @classmethod
    def get_supported_act_dtypes(cls) -&gt; list[torch.dtype]:
        return [torch.half]

    @classmethod
    def get_min_capability(cls) -&gt; int:
        return 60

    @classmethod
    def get_config_filenames(cls) -&gt; list[str]:
        return [&quot;quantize_config.json&quot;]

    @classmethod
    def from_config(cls, config: dict) -&gt; &quot;QTIPConfig&quot;:
        td_x = cls.get_from_keys(config, [&quot;td_x&quot;])
        td_y = cls.get_from_keys(config, [&quot;td_y&quot;])
        L = cls.get_from_keys(config, [&quot;L&quot;])
        K = cls.get_from_keys(config, [&quot;K&quot;])
        V = cls.get_from_keys(config, [&quot;V&quot;])
        tlut_bits = cls.get_from_keys(config, [&quot;tlut_bits&quot;])
        decode_mode = cls.get_from_keys(config, [&quot;decode_mode&quot;])
        scale = cls.get_from_keys_or(config, [&quot;scale&quot;], default=32.0)
        return cls(td_x, td_y, L, K, V, tlut_bits, decode_mode, scale)

    def get_quant_method(self, layer: nn.Module,
                         prefix: str) -&gt; &quot;QTIPLinearMethod&quot;:
        return QTIPLinearMethod(self)


class QTIPLinearMethod(LinearMethodBase):
    &quot;&quot;&quot;
    QTIP linear layer quantization method
    &quot;&quot;&quot;

    def __init__(self, quant_config: QTIPConfig):
        self.cfg = quant_config
        # Build lookup table (codebook)
        self.cb = bitshift_codebook(L=self.cfg.L,
                                    K=self.cfg.K,
                                    V=self.cfg.V,
                                    tlut_bits=self.cfg.tlut_bits,
                                    decode_mode=self.cfg.decode_mode)
        self.scale = self.cfg.scale

    def create_weights(self, layer: nn.Module, input_size_per_partition: int,
                       output_partition_sizes: list[int], input_size: int,
                       output_size: int, params_dtype: torch.dtype,
                       **extra_weight_attrs):
        output_size_per_partition = sum(output_partition_sizes)
        pack_factor = self.cfg.pack_factor
        assert input_size_per_partition % pack_factor == 0, \
            &quot;input size must be multiple of pack_factor&quot;
        rows = input_size_per_partition // pack_factor

        qweight = PackedvLLMParameter(
            data=torch.empty(rows,
                             output_size_per_partition,
                             dtype=torch.int32),
            input_dim=0,
            output_dim=1,
            packed_dim=0,
            packed_factor=pack_factor,
            weight_loader=lambda p, w: p.data.copy_(w))
        layer.register_parameter(&quot;qweight&quot;, qweight)

        # Register SU and SV
        layer.register_buffer(
            &quot;SU&quot;, torch.ones(input_size_per_partition, dtype=params_dtype))
        layer.register_buffer(
            &quot;SV&quot;, torch.ones(output_size_per_partition, dtype=torch.float32))

    def process_weights_after_loading(self, layer: nn.Module) -&gt; None:
        &quot;&quot;&quot;
        Unpack loaded quantized indices and restore to int32 index matrix
        &quot;&quot;&quot;
        packed = layer.qweight.data
        unpacked = self.cb.unpack_trellis(packed, self.cfg.pack_factor)
        layer.qweight.data = unpacked.to(torch.int32)

    def apply(self,
              layer: nn.Module,
              x: torch.Tensor,
              bias: Optional[torch.Tensor] = None) -&gt; torch.Tensor:
        output = bitshift_gemm(input=x,
                               trellis=layer.qweight,
                               codebook=self.cb,
                               td_x=self.cfg.td_x,
                               td_y=self.cfg.td_y,
                               scale=self.scale,
                               SU=layer.SU,
                               SV=layer.SV)
        if bias is not None:
            output = output + bias
        return output


</code></pre>
<h2 id="step-4-define-the-bitshift-codebook-in-_custom_opspy">Step 4: Define the Bitshift Codebook (in _custom_ops.py)<a class="headerlink" href="#step-4-define-the-bitshift-codebook-in-_custom_opspy" title="Permanent link">&para;</a></h2>
<p>Implement the <code>bitshift_codebook</code> class that handles decoding quantized weights:</p>
<pre><code class="language-python">
import numpy as np
import torch.nn as nn

class bitshift_codebook(nn.Module):
    def __init__(self,
                 L: int,
                 K: int,
                 V: int,
                 tlut_bits: int,
                 decode_mode: str,
                 tlut: torch.Tensor = None):
        super().__init__()
        self.L = L
        self.K = K
        self.V = V
        self.tlut_bits = tlut_bits
        self.decode_mode = decode_mode

        levels = 1 &lt;&lt; L

        if decode_mode == 'lut':
            # Use externally provided tlut or random initialization
            if tlut is None:
                assert tlut_bits == L
                tbl = torch.randn(levels, V, dtype=torch.float16)
            else:
                tbl = tlut
            # Store as [V, levels]
            self.register_buffer('lut', tbl.T.contiguous())

        elif decode_mode == '1mad':
            assert V == 1
            vals = decode_1mad(torch.arange(levels, device='cpu'))
            self.register_buffer('lut', vals.unsqueeze(0).to(torch.float16))

        elif decode_mode == '2mad':
            assert V == 1
            vals = decode_2mad(torch.arange(levels, device='cpu'))
            self.register_buffer('lut', vals.unsqueeze(0).to(torch.float16))

        elif decode_mode == '3inst':
            assert V == 1
            vals = decode_3inst(torch.arange(levels, device='cpu'))
            self.register_buffer('lut', vals.unsqueeze(0).to(torch.float16))

        elif decode_mode == 'quantlut':
            assert tlut is not None
            tbl = quantlut(tlut, L, tlut_bits)
            self.register_buffer('lut', tbl)

        elif decode_mode == 'quantlut_sym':
            assert tlut is not None
            tbl = quantlut_sym(tlut, L, tlut_bits)
            self.register_buffer('lut', tbl)

        else:
            raise ValueError(f&quot;Unsupported decode_mode: {decode_mode!r}&quot;)

    def recons(self, encoded: torch.Tensor) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        encoded: LongTensor([num_blocks, td_x*td_y])
        returns: FloatTensor([V, num_blocks, td_x*td_y])
        &quot;&quot;&quot;
        # encoded.long() ensures proper dtype, then gather from lut
        # self.lut shape = [V, levels]
        return self.lut[:, encoded.long()]

    def unpack_trellis(self, packed: torch.Tensor, pack_factor: int) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        Unpack trellis indices from packed format

        Args:
            packed: Packed tensor of shape [rows, cols]
            pack_factor: Number of elements per block

        Returns:
            Unpacked tensor of shape [rows, cols*pack_factor]
        &quot;&quot;&quot;
        rows, cols = packed.shape
        unpacked = torch.zeros((rows * pack_factor, cols), 
                              device=packed.device, 
                              dtype=torch.int32)

        # Unpack each element based on the bit pattern
        for i in range(pack_factor):
            mask = (1 &lt;&lt; self.L) - 1
            shift = i * self.L
            unpacked[i::pack_factor, :] = (packed &gt;&gt; shift) &amp; mask

        return unpacked
</code></pre>
<h2 id="step-5-implement-decoding-functions-for-qtip-in-_custom_opspy">Step 5: Implement Decoding Functions for QTIP (in _custom_ops.py)<a class="headerlink" href="#step-5-implement-decoding-functions-for-qtip-in-_custom_opspy" title="Permanent link">&para;</a></h2>
<p>Add these decoding functions to support different modes:</p>
<pre><code class="language-python">def decode_1mad(x: torch.Tensor) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Decode using 1-MAD (Multiply-Add) method.
    This decoding uses a single multiply-add operation.
    &quot;&quot;&quot;
    x = x.to(torch.int64) &amp; ((1 &lt;&lt; 32) - 1)
    x = x * 34038481 + 76625530
    x = x &amp; ((1 &lt;&lt; 32) - 1)
    y = (x &amp; 0xFF) + ((x &gt;&gt; 8) &amp; 0xFF) + ((x &gt;&gt; 16) &amp; 0xFF) + ((x &gt;&gt; 24) &amp; 0xFF)
    y = y - 510
    return (y.to(torch.float32) / 147.800537109375)

def decode_2mad(x: torch.Tensor) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Decode using 2-MAD method.
    Uses two multiply-add operations for potentially better accuracy.
    &quot;&quot;&quot;
    x = x.to(torch.int64) &amp; ((1 &lt;&lt; 32) - 1)
    x = x * 264435761 + 1013904223
    x = x &amp; ((1 &lt;&lt; 32) - 1)
    x = ((x * 1664525) &gt;&gt; 32) + x
    x = x &amp; ((1 &lt;&lt; 32) - 1)
    y = (x &amp; 0xFF) + ((x &gt;&gt; 8) &amp; 0xFF) + ((x &gt;&gt; 16) &amp; 0xFF) + ((x &gt;&gt; 24) &amp; 0xFF)
    y = y - 510
    return (y.to(torch.float32) / 147.800537109375)

def decode_3inst(x: torch.Tensor) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Decode using 3-instruction method.
    This uses floating point bit manipulation for high precision.
    &quot;&quot;&quot;
    def bfe16_to_fp16(z: torch.Tensor) -&gt; torch.Tensor:
        arr = z.to(torch.int32)
        mask = arr &gt;= (1 &lt;&lt; 15)
        arr[mask] -= (1 &lt;&lt; 16)

        tmp = arr.to(torch.int16).cpu().numpy().view(np.float16)
        return torch.from_numpy(tmp).to(z.device)

    a, b = 89226354, 64248484
    fpmask = 996162400
    z = x.to(torch.int64) &amp; ((1 &lt;&lt; 32) - 1)
    z = z * a + b
    mask = ((1 &lt;&lt; 15) | ((1 &lt;&lt; 12) - 1)) &lt;&lt; 16
    res = (mask &amp; z) ^ fpmask
    top = bfe16_to_fp16(res &gt;&gt; 16)
    bot = bfe16_to_fp16(res &amp; 0xFFFF)
    return (top + bot).float()

def quantlut(tlut: torch.Tensor, L: int, nbits: int) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Create a quantized lookup table from a provided table.
    &quot;&quot;&quot;
    lut = torch.arange(1 &lt;&lt; L, device=tlut.device)
    lut = (lut + 1) * lut
    lut = (lut &gt;&gt; (16 - nbits)) &amp; ((1 &lt;&lt; nbits) - 1)
    return tlut[lut].T.contiguous()

def quantlut_sym(tlut: torch.Tensor, L: int, nbits: int) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Create a symmetric quantized lookup table.
    &quot;&quot;&quot;
    lut = torch.arange(1 &lt;&lt; L, device=tlut.device)
    lut = (lut + 1) * lut
    sign = 1 - ((lut &gt;&gt; 15) &amp; 1) * 2
    lut = (lut &gt;&gt; (16 - nbits - 1)) &amp; ((1 &lt;&lt; nbits) - 1)
    out = tlut[lut]
    out[:, 0] = out[:, 0] * sign
    return out.T.contiguous()
</code></pre>
<h2 id="step-6-implement-the-bitshift-gemm-function-in-_custom_opspy">Step 6: Implement the Bitshift GEMM Function (in _custom_ops.py)<a class="headerlink" href="#step-6-implement-the-bitshift-gemm-function-in-_custom_opspy" title="Permanent link">&para;</a></h2>
<p>This function performs the matrix multiplication with the quantized weights:</p>
<pre><code class="language-python">def bitshift_gemm(
    input: torch.Tensor,
    trellis: torch.Tensor,
    codebook: bitshift_codebook,
    td_x: int,
    td_y: int,
    scale: float,
    SU: torch.Tensor,
    SV: torch.Tensor
) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Python fallback for QTIP Bitshift GEMM.

    Steps:
      1. Decode each block via codebook.recons → [V, num_blocks, block_size]
      2. (V must be 1 in fallback) take recons[0] → [num_blocks, block_size]
      3. Reshape &amp; transpose to reconstruct full weight matrix hatW [m, n]
      4. Compute input @ hatW^T and divide by scale → output [B, n]

    Args:
        input:    [B, m]  activation tensor
        trellis:  [num_blocks, td_x*td_y]  block-wise index matrix
        codebook: bitshift_codebook instance (with .recons method)
        td_x:     block row size
        td_y:     block col size
        scale:    dequantization scale factor
        SU:       scale factor for input
        SV:       scale factor for output

    Returns:
        output: [B, n]  result of dequantized GEMM
    &quot;&quot;&quot;
    B, m = input.shape
    input = input.to(torch.float32) * SU  # ← SU corrects input

    # decode
    recons = codebook.recons(trellis)
    assert recons.shape[0] == 1
    recons = recons[0]

    row_blocks = m // td_x
    col_blocks = recons.shape[0] // row_blocks
    n = col_blocks * td_y

    hatW = (
        recons
        .view(row_blocks, col_blocks, td_x, td_y)
        .transpose(1, 2)
        .reshape(m, n)
    )

    out = input.matmul(hatW.T)  # [B, n]
    return (out * SV * scale).to(input.dtype)  # ← SV corrects output
</code></pre>
<h2 id="step-7-register-qtip-in-the-quantization-methods-registryin-vllmmodel_executorlayersquantizationinitpy">Step 7: Register QTIP in the Quantization Methods Registry(in vllm/model_executor/layers/quantization/<strong>init</strong>.py)<a class="headerlink" href="#step-7-register-qtip-in-the-quantization-methods-registryin-vllmmodel_executorlayersquantizationinitpy" title="Permanent link">&para;</a></h2>
<p>Update the <code>__init__.py</code> file in the quantization directory to include QTIP:</p>
<pre><code class="language-python"># SPDX-License-Identifier: Apache-2.0

from typing import Literal, get_args

from vllm.model_executor.layers.quantization.base_config import (
    QuantizationConfig)

QuantizationMethods = Literal[
    &quot;aqlm&quot;,
    &quot;awq&quot;,
    &quot;deepspeedfp&quot;,
    &quot;tpu_int8&quot;,
    &quot;fp8&quot;,
    &quot;ptpc_fp8&quot;,
    &quot;fbgemm_fp8&quot;,
    &quot;modelopt&quot;,
    &quot;nvfp4&quot;,
    &quot;marlin&quot;,
    &quot;bitblas&quot;,
    &quot;gguf&quot;,
    &quot;gptq_marlin_24&quot;,
    &quot;gptq_marlin&quot;,
    &quot;gptq_bitblas&quot;,
    &quot;awq_marlin&quot;,
    &quot;gptq&quot;,
    &quot;compressed-tensors&quot;,
    &quot;bitsandbytes&quot;,
    &quot;qqq&quot;,
    &quot;hqq&quot;,
    &quot;experts_int8&quot;,
    &quot;neuron_quant&quot;,
    &quot;ipex&quot;,
    &quot;quark&quot;,
    &quot;moe_wna16&quot;,
    &quot;qtip&quot;,
    &quot;torchao&quot;,
]
QUANTIZATION_METHODS: list[str] = list(get_args(QuantizationMethods))

# The customized quantization methods which will be added to this dict.
_CUSTOMIZED_METHOD_TO_QUANT_CONFIG = {}


def register_quantization_config(quantization: str):
    &quot;&quot;&quot;Register a customized vllm quantization config.

    When a quantization method is not supported by vllm, you can register a customized
    quantization config to support it.

    Args:
        quantization (str): The quantization method name.

    Examples:
        &gt;&gt;&gt; from vllm.model_executor.layers.quantization import register_quantization_config
        &gt;&gt;&gt; from vllm.model_executor.layers.quantization import get_quantization_config
        &gt;&gt;&gt; from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
        &gt;&gt;&gt;
        &gt;&gt;&gt; @register_quantization_config(&quot;my_quant&quot;)
        ... class MyQuantConfig(QuantizationConfig):
        ...     pass
        &gt;&gt;&gt;
        &gt;&gt;&gt; get_quantization_config(&quot;my_quant&quot;)
        &lt;class 'MyQuantConfig'&gt;
    &quot;&quot;&quot;  # noqa: E501

    def _wrapper(quant_config_cls):
        if quantization in QUANTIZATION_METHODS:
            raise ValueError(
                f&quot;The quantization method `{quantization}` is already exists.&quot;)
        if not issubclass(quant_config_cls, QuantizationConfig):
            raise ValueError(&quot;The quantization config must be a subclass of &quot;
                             &quot;`QuantizationConfig`.&quot;)
        _CUSTOMIZED_METHOD_TO_QUANT_CONFIG[quantization] = quant_config_cls
        QUANTIZATION_METHODS.append(quantization)
        return quant_config_cls

    return _wrapper


def get_quantization_config(quantization: str) -&gt; type[QuantizationConfig]:
    if quantization not in QUANTIZATION_METHODS:
        raise ValueError(f&quot;Invalid quantization method: {quantization}&quot;)

    # lazy import to avoid triggering `torch.compile` too early
    from vllm.model_executor.layers.quantization.quark.quark import QuarkConfig

    from .aqlm import AQLMConfig
    from .awq import AWQConfig
    from .awq_marlin import AWQMarlinConfig
    from .bitblas import BitBLASConfig
    from .bitsandbytes import BitsAndBytesConfig
    from .compressed_tensors.compressed_tensors import (  # noqa: E501
        CompressedTensorsConfig)
    from .deepspeedfp import DeepSpeedFPConfig
    from .experts_int8 import ExpertsInt8Config
    from .fbgemm_fp8 import FBGEMMFp8Config
    from .fp8 import Fp8Config
    from .gguf import GGUFConfig
    from .gptq import GPTQConfig
    from .gptq_bitblas import GPTQBitBLASConfig
    from .gptq_marlin import GPTQMarlinConfig
    from .gptq_marlin_24 import GPTQMarlin24Config
    from .hqq_marlin import HQQMarlinConfig
    from .ipex_quant import IPEXConfig
    from .marlin import MarlinConfig
    from .modelopt import ModelOptFp8Config, ModelOptNvFp4Config
    from .moe_wna16 import MoeWNA16Config
    from .neuron_quant import NeuronQuantConfig
    from .ptpc_fp8 import PTPCFp8Config
    from .qqq import QQQConfig
    from .qtip import QTIPConfig
    from .torchao import TorchAOConfig
    from .tpu_int8 import Int8TpuConfig

    method_to_config: dict[str, type[QuantizationConfig]] = {
        &quot;aqlm&quot;: AQLMConfig,
        &quot;awq&quot;: AWQConfig,
        &quot;deepspeedfp&quot;: DeepSpeedFPConfig,
        &quot;tpu_int8&quot;: Int8TpuConfig,
        &quot;fp8&quot;: Fp8Config,
        &quot;fbgemm_fp8&quot;: FBGEMMFp8Config,
        &quot;modelopt&quot;: ModelOptFp8Config,
        &quot;nvfp4&quot;: ModelOptNvFp4Config,
        &quot;marlin&quot;: MarlinConfig,
        &quot;bitblas&quot;: BitBLASConfig,
        &quot;gguf&quot;: GGUFConfig,
        &quot;gptq_marlin_24&quot;: GPTQMarlin24Config,
        &quot;gptq_marlin&quot;: GPTQMarlinConfig,
        &quot;gptq_bitblas&quot;: GPTQBitBLASConfig,
        &quot;awq_marlin&quot;: AWQMarlinConfig,
        &quot;gptq&quot;: GPTQConfig,
        &quot;compressed-tensors&quot;: CompressedTensorsConfig,
        &quot;bitsandbytes&quot;: BitsAndBytesConfig,
        &quot;ptpc_fp8&quot;: PTPCFp8Config,
        &quot;qqq&quot;: QQQConfig,
        &quot;hqq&quot;: HQQMarlinConfig,
        &quot;experts_int8&quot;: ExpertsInt8Config,
        &quot;neuron_quant&quot;: NeuronQuantConfig,
        &quot;ipex&quot;: IPEXConfig,
        &quot;quark&quot;: QuarkConfig,
        &quot;moe_wna16&quot;: MoeWNA16Config,
        &quot;qtip&quot;: QTIPConfig,
        &quot;torchao&quot;: TorchAOConfig,
    }
    # Update the `method_to_config` with customized quantization methods.
    method_to_config.update(_CUSTOMIZED_METHOD_TO_QUANT_CONFIG)

    return method_to_config[quantization]


__all__ = [
    &quot;QuantizationConfig&quot;,
    &quot;QuantizationMethods&quot;,
    &quot;get_quantization_config&quot;,
    &quot;QUANTIZATION_METHODS&quot;,
]
</code></pre>
<h2 id="step-8-test-the-integration-in-testskernelsquantizationtest_qtippy">Step 8: Test the Integration (in tests/kernels/quantization/test_qtip.py)<a class="headerlink" href="#step-8-test-the-integration-in-testskernelsquantizationtest_qtippy" title="Permanent link">&para;</a></h2>
<p>Create a test file to validate your implementation:</p>
<pre><code class="language-python"># SPDX-License-Identifier: Apache-2.0

import pytest
import torch

from vllm import _custom_ops as ops

HIDDEN_SIZES = [4096]
OUT_SIZES = [4096]
L_VALUES = [16]
K_VALUES = [3]
V_VALUES = [1]


@pytest.mark.parametrize(&quot;hidden_size&quot;, HIDDEN_SIZES)
@pytest.mark.parametrize(&quot;out_size&quot;, OUT_SIZES)
@pytest.mark.parametrize(&quot;L&quot;, L_VALUES)
@pytest.mark.parametrize(&quot;K&quot;, K_VALUES)
@pytest.mark.parametrize(&quot;V&quot;, V_VALUES)
def test_bitshift_codebook(hidden_size, out_size, L, K, V):
    td_x, td_y = 8, 8
    block_size = td_x * td_y
    row_blocks = hidden_size // td_x
    col_blocks = out_size // td_y
    num_blocks = row_blocks * col_blocks

    codebook = ops.bitshift_codebook(
        L=L,
        K=K,
        V=V,
        tlut_bits=L,
        decode_mode=&quot;lut&quot;
    )

    encoded = torch.randint(
        0, 2**L,
        (num_blocks, block_size),
        device='cpu', 
        dtype=torch.long
    )

    decoded = codebook.recons(encoded)

    assert decoded.shape == (V, num_blocks, block_size)


@pytest.mark.parametrize(&quot;hidden_size&quot;, HIDDEN_SIZES)
@pytest.mark.parametrize(&quot;out_size&quot;, OUT_SIZES)
@pytest.mark.parametrize(&quot;L&quot;, L_VALUES)
@pytest.mark.parametrize(&quot;K&quot;, K_VALUES)
@pytest.mark.parametrize(&quot;V&quot;, V_VALUES)
def test_bitshift_gemm(hidden_size, out_size, L, K, V):
    td_x = 8
    td_y = 8
    block_size = td_x * td_y

    m = hidden_size
    n = out_size
    row_blocks = m // td_x
    col_blocks = n // td_y
    num_blocks = row_blocks * col_blocks

    codebook = ops.bitshift_codebook(
        L=L,
        K=K,
        V=V,
        tlut_bits=L,
        decode_mode=&quot;lut&quot;
    )

    x = torch.rand((1, m), device='cpu', dtype=torch.float16)

    trellis = torch.randint(
        0, 2 ** L,
        (num_blocks, block_size),
        device='cpu',
        dtype=torch.long
    )

    SU = torch.ones(m, device='cpu', dtype=torch.float16)
    SV = torch.ones(n, device='cpu', dtype=torch.float16)

    output = ops.bitshift_gemm(
        input=x,
        trellis=trellis,
        codebook=codebook,
        td_x=td_x,
        td_y=td_y,
        scale=32.0,
        SU=SU,
        SV=SV
    )

    assert output.shape == (1, n)
</code></pre>
<h2 id="step-8-test-the-config-in-testsquantizationtest_qtip_configpy">Step 8: Test the Config (in tests\quantization\test_qtip_config.py)<a class="headerlink" href="#step-8-test-the-config-in-testsquantizationtest_qtip_configpy" title="Permanent link">&para;</a></h2>
<pre><code class="language-python"># SPDX-License-Identifier: Apache-2.0
&quot;&quot;&quot;Tests for QTIP quantization configuration.

Run `pytest tests/quantization/test_qtip_config.py`.
&quot;&quot;&quot;

import pytest
import torch

from vllm.model_executor.layers.quantization.qtip import (QTIPConfig,
                                                          QTIPLinearMethod)


def test_qtip_config_creation():
    &quot;&quot;&quot;Test QTIP configuration creation and validation.&quot;&quot;&quot;

    config = QTIPConfig(td_x=8,
                        td_y=8,
                        L=16,
                        K=2,
                        V=2,
                        tlut_bits=16,
                        decode_mode=&quot;1mad&quot;,
                        scale=32.0)


    assert config.td_x == 8
    assert config.td_y == 8
    assert config.L == 16
    assert config.K == 2
    assert config.V == 2
    assert config.tlut_bits == 16
    assert config.decode_mode == &quot;1mad&quot;
    assert config.scale == 32.0
    assert config.pack_factor == 64  # td_x * td_y


    config_dict = {
        &quot;td_x&quot;: 8,
        &quot;td_y&quot;: 8,
        &quot;L&quot;: 16,
        &quot;K&quot;: 2,
        &quot;V&quot;: 2,
        &quot;tlut_bits&quot;: 16,
        &quot;decode_mode&quot;: &quot;1mad&quot;,
        &quot;scale&quot;: 32.0
    }
    config_from_dict = QTIPConfig.from_config(config_dict)
    assert config_from_dict.td_x == config.td_x
    assert config_from_dict.td_y == config.td_y
    assert config_from_dict.L == config.L
    assert config_from_dict.K == config.K
    assert config_from_dict.V == config.V
    assert config_from_dict.tlut_bits == config.tlut_bits
    assert config_from_dict.decode_mode == config.decode_mode
    assert config_from_dict.scale == config.scale


def test_qtip_config_methods():
    &quot;&quot;&quot;Test QTIP configuration methods.&quot;&quot;&quot;
    config = QTIPConfig(td_x=8,
                        td_y=8,
                        L=16,
                        K=2,
                        V=2,
                        tlut_bits=16,
                        decode_mode=&quot;1mad&quot;,
                        scale=32.0)


    assert config.get_name() == &quot;qtip&quot;


    assert torch.half in config.get_supported_act_dtypes()


    assert config.get_min_capability() == 60


    assert &quot;quantize_config.json&quot; in config.get_config_filenames()




# needs cuda, currently using cpu
# @pytest.mark.parametrize(
#     &quot;model&quot;,
#     [
#         &quot;meta-llama/Llama-2-7b-hf&quot;,  
#     ])
# def test_qtip_inference(vllm_runner, model, monkeypatch):
#     &quot;&quot;&quot;Test inference with QTIP quantization.&quot;&quot;&quot;

#     monkeypatch.setenv(&quot;VLLM_USE_V1&quot;, &quot;0&quot;)


#     qtip_config = {
#         &quot;quant_method&quot;: &quot;qtip&quot;,
#         &quot;td_x&quot;: 8,
#         &quot;td_y&quot;: 8,
#         &quot;L&quot;: 16,
#         &quot;K&quot;: 2,
#         &quot;V&quot;: 2,
#         &quot;tlut_bits&quot;: 16,
#         &quot;decode_mode&quot;: &quot;1mad&quot;,
#         &quot;scale&quot;: 32.0
#     }


#     with vllm_runner(model_name=model,
#                      quantization=&quot;qtip&quot;,
#                      enforce_eager=True,
#                      ) as llm:

#         model = llm.model.llm_engine.model_executor.driver_worker.model_runner.model
#         layer = model.model.layers[0]

#         assert isinstance(layer.self_attn.qkv_proj.quant_method,
#                           QTIPLinearMethod)

#         output = llm.generate_greedy(&quot;Hello my name is&quot;, max_tokens=20)
#         assert output

</code></pre>
<h2 id="step-9-testing-command-in-the-workspace-of-docker">Step 9: Testing command (in the workspace of docker)<a class="headerlink" href="#step-9-testing-command-in-the-workspace-of-docker" title="Permanent link">&para;</a></h2>
<pre><code class="language-python">pip install -r requirements/dev.txt
pytest tests/kernels/quantization/test_qtip.py
pytest tests/quantization/test_qtip_config.py
</code></pre>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="May 24, 2025 11:20:49">May 24, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <p style="text-align: center; font-size: 1.3em; color: #777;">
  This documentation was created by <strong>Runxin Shao</strong> and last updated on
  <span style="white-space: nowrap;"></span>
</p>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>